---
Fecha de creación: 2025-08-06 16:59
Fecha de Modificación: 2025-08-06 16:59
tags:
  - inteligencia-artificial
Topic:
  - NLP
---

## 📚 Idea/Concepto 


La tokenización consiste en la división de un texto en tokens, los cuales pueden ser palabras, subpalabras o símbolos, algunos de sus algoritmos incluyen BPE y WordPiece. Es el paso inicial antes de la conversión a representaciones numéricas utilizadas por los modelos de machine learning, para esto son asignados identificadores numéricos para luego ser mapeados a una matriz de embeddings aprendida con un vocabulario predefinido. Además, la tokenización de subpalabras es esencial para reducir el tamaño del vocabulario y manejar palabras desconocidas. Estas subpalabras pueden ser creadas por algoritmos como frecuencia de pares. Dentro de sus limitaciones se encuentra que la secuencia de tokens está limitada por la ventana de contexto, lo que afecta arquitecturas Transformer.

## 🔗 Connections

- [[Redes Neuronales]]
- [[Embeddings]]
