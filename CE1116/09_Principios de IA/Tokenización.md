---
Fecha de creaci贸n: 2025-08-06 16:59
Fecha de Modificaci贸n: 2025-08-06 16:59
tags:
  - inteligencia-artificial
Topic:
  - NLP
---

##  Idea/Concepto 


La tokenizaci贸n consiste en la divisi贸n de un texto en tokens, los cuales pueden ser palabras, subpalabras o s铆mbolos, algunos de sus algoritmos incluyen BPE y WordPiece. Es el paso inicial antes de la conversi贸n a representaciones num茅ricas utilizadas por los modelos de machine learning, para esto son asignados identificadores num茅ricos para luego ser mapeados a una matriz de embeddings aprendida con un vocabulario predefinido. Adem谩s, la tokenizaci贸n de subpalabras es esencial para reducir el tama帽o del vocabulario y manejar palabras desconocidas. Estas subpalabras pueden ser creadas por algoritmos como frecuencia de pares. Dentro de sus limitaciones se encuentra que la secuencia de tokens est谩 limitada por la ventana de contexto, lo que afecta arquitecturas Transformer.

##  Connections

- [[Redes Neuronales]]
- [[Embeddings]]
