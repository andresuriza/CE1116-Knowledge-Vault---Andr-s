---
Fecha de creaci贸n: 2025-08-06 16:59
Fecha de Modificaci贸n: 2025-08-06 16:59
tags:
  - inteligencia-artificial
Topic:
  - LLM
---

##  Idea/Concepto 


Los mecanismos de atenci贸n en los Transformers buscan dirigir la atenci贸n a los tokens m谩s importantes para contextualizar la respuesta, esto mediante los vectores de Query, Key y Value, la relevancia del token se determina mediante una compatibilidad entre query y key, se realiza un producto punto entre Query y Key, el factor escalado de este resultado seguido de la funci贸n Softmax determina su peso correspondiente, generando as铆 una matriz de atenci贸n la cual pondera el vector Value, sum谩ndolo al embedding original, el cual ya contiene la codificaci贸n posicional, para actualizarlos y enriquecer el contexto. Estos mecanismos tambi茅n utilizan la atenci贸n multi-cabeza para procesar relaciones contextuales en paralelo y simult谩neamente.

##  Connections

- [[Tokenizaci贸n]]
- [[Redes Neuronales]]
