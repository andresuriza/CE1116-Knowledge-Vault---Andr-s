---
Fecha de creación: 2025-08-06 16:59
Fecha de Modificación: 2025-08-06 16:59
tags:
  - inteligencia-artificial
Topic:
  - LLM
---

## 📚 Idea/Concepto 


Los embeddings son vectores numéricos de alta dimensión utilizados como parámetros aprendidos por los modelos de machine learning, cuyo propósito es codificar significado semántico y contextual de entradas como texto, imágenes o audio. Estos significados se posicionan cerca unos de otros en el espacio vectorial dependiendo de que tan similares son. Sirven como la representación inicial de los tokens antes de ser refinados por capas, además de que se ajustan durante el entrenamiento utilizando el algoritmo de backpropagation. Los embeddings pueden ser contextuales si se utilizan en arquitecturas como Transformers, donde el significado de las palabras depende de aquellas que la rodean y su secuencia es definida por la codificación posicional.

## 🔗 Connections

- [[Tokenización]]
- [[Redes Neuronales]]
- [[Mecanismo de atención utilizado en la arquitectura de Transformers]]
